{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebd5b96",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a868c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('IMDB dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e8a11",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c2cba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_cleaned'] = df['review'].apply(lambda x: x.replace('<br />', ''))\n",
    "# whitespace removal\n",
    "df['review_cleaned'] = df['review_cleaned'].replace(r'\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83adfa8",
   "metadata": {},
   "source": [
    "## Encode sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e183b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_encoded'] = df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ec69d",
   "metadata": {},
   "source": [
    "## Tokenize data\n",
    "\n",
    "With the data prepared it needs to be tokenized in preparation for BERT. This uses the `BertTokenizer` from Hugging Face `transformers` to do so.\n",
    "\n",
    "We use the `bert-case-uncased` tokenizer which ignores the casing of the reviews. This may or may not be appropriate. There could be 'implied' meaning in such usage however internet users are also historically notorious for poor casing in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2211eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Example tokenization\n",
    "# sample_sentence = 'I liked this movie'\n",
    "# token_ids = tokenizer.encode(sample_sentence, return_tensors='np')[0]\n",
    "# print(f'Token IDs: {token_ids}')\n",
    "# # Convert the token IDs back to tokens to reveal the special tokens added\n",
    "# tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "# print(f'Tokens   : {tokens}')\n",
    "\n",
    "token_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for review in df['review_cleaned']:\n",
    "  # We have a large number of padding tokens due to short reviews. \n",
    "  # Using `encode_plus` lets us add an attention mask to ignore all\n",
    "  # these padding tokens therefore avoiding attending to these.\n",
    "  batch_encoder = tokenizer.encode_plus(\n",
    "    review,\n",
    "    max_length = 512,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_tensors = 'pt'\n",
    "  )\n",
    "\n",
    "  token_ids.append(batch_encoder['input_ids'])\n",
    "  attention_masks.append(batch_encoder['attention_mask'])\n",
    "\n",
    "# Converts our lists to PyToch tensors\n",
    "token_ids = torch.cat(token_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e37688",
   "metadata": {},
   "source": [
    "## Preparing training & evaluation datasets\n",
    "\n",
    "See [3.3 - Create the Train and Validation Dataloaders](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4590134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "val_size = 0.1\n",
    "\n",
    "train_ids, val_ids = train_test_split(\n",
    "  token_ids,\n",
    "  test_size=val_size,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "train_masks, val_masks = train_test_split(\n",
    "  attention_masks,\n",
    "  test_size=val_size,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "labels = torch.tensor(df['sentiment_encoded'].values)\n",
    "train_labels, val_labels = train_test_split(\n",
    "  labels,\n",
    "  test_size=val_size,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)\n",
    "val_data = TensorDataset(val_ids, val_masks, val_labels)\n",
    "val_dataloader = DataLoader(val_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7316867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "  'bert-base-uncased',\n",
    "  num_labels=2 # positive or negative\n",
    ")\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "# The optimizer aims to converage weights and bias towards an optimal.\n",
    "optimizer = AdamW(model.parameters())\n",
    "# Language models typically use the cross entropy loss function.\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# The scheduler determines the size of changes to weights and biases.\n",
    "# Since we start with random parameters then big changes early on can\n",
    "# often be beneficial to converge. As training progresses, changes should\n",
    "# be smaller towards the convergance. This is what the linear scheduler aims\n",
    "# to do.\n",
    "num_training_steps = EPOCHS * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(\"Using Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68e836",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        batch_token_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss, logits = model(\n",
    "            batch_token_ids,\n",
    "            token_type_ids = None,\n",
    "            attention_mask=batch_attention_mask,\n",
    "            labels=batch_labels,\n",
    "            return_dict=False)\n",
    "\n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    average_train_loss = training_loss / len(train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
