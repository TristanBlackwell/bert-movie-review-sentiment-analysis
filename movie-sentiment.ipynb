{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebd5b96",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a868c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('IMDB dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e8a11",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2cba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_cleaned'] = df['review'].apply(lambda x: x.replace('<br />', ''))\n",
    "# whitespace removal\n",
    "df['review_cleaned'] = df['review_cleaned'].replace(r'\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83adfa8",
   "metadata": {},
   "source": [
    "## Encode sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e183b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_encoded'] = df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ec69d",
   "metadata": {},
   "source": [
    "## Tokenize data\n",
    "\n",
    "With the data prepared it needs to be tokenized in preparation for BERT. This uses the `BertTokenizer` from Hugging Face `transformers` to do so.\n",
    "\n",
    "We use the `bert-case-uncased` tokenizer which ignores the casing of the reviews. This may or may not be appropriate. There could be 'implied' meaning in such usage however internet users are also historically notorious for poor casing in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2211eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Example tokenization\n",
    "# sample_sentence = 'I liked this movie'\n",
    "# token_ids = tokenizer.encode(sample_sentence, return_tensors='np')[0]\n",
    "# print(f'Token IDs: {token_ids}')\n",
    "# # Convert the token IDs back to tokens to reveal the special tokens added\n",
    "# tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "# print(f'Tokens   : {tokens}')\n",
    "\n",
    "token_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for review in df['review_cleaned']:\n",
    "  # We have a large number of padding tokens due to short reviews.\n",
    "  # Using `encode_plus` lets us add an attention mask to ignore all\n",
    "  # these padding tokens therefore avoiding attending to these.\n",
    "  batch_encoder = tokenizer.encode_plus(\n",
    "    review,\n",
    "    max_length = 512,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_tensors = 'pt'\n",
    "  )\n",
    "\n",
    "  token_ids.append(batch_encoder['input_ids'])\n",
    "  attention_masks.append(batch_encoder['attention_mask'])\n",
    "\n",
    "# Converts our lists to PyToch tensors\n",
    "token_ids = torch.cat(token_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e37688",
   "metadata": {},
   "source": [
    "## Preparing training & evaluation datasets\n",
    "\n",
    "See [3.3 - Create the Train and Validation Dataloaders](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "val_size = 0.1\n",
    "\n",
    "train_ids, val_ids = train_test_split(\n",
    "  token_ids,\n",
    "  test_size=val_size,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "train_masks, val_masks = train_test_split(\n",
    "  attention_masks,\n",
    "  test_size=val_size,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "labels = torch.tensor(df['sentiment_encoded'].values)\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    token_ids,\n",
    "    attention_masks,\n",
    "    labels,\n",
    "    test_size=0.1,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_data   = TensorDataset(val_ids,   val_masks,   val_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)\n",
    "val_dataloader   = DataLoader(val_data,   batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7316867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "  'bert-base-uncased',\n",
    "  num_labels=2 # positive or negative\n",
    ")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Using Device: \", device)\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "# The optimizer aims to converage weights and bias towards an optimal.\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "# Language models typically use the cross entropy loss function.\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# The scheduler determines the size of changes to weights and biases.\n",
    "# Since we start with random parameters then big changes early on can\n",
    "# often be beneficial to converge. As training progresses, changes should\n",
    "# be smaller towards the convergance. This is what the linear scheduler aims\n",
    "# to do.\n",
    "num_training_steps = EPOCHS * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68e836",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Apple M4 chip CPU training is horrendously slow (no suprise!). Takes X on a T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    batches = len(train_dataloader);\n",
    "    print(f\"\\nEpoch {epoch} — total batches: {batches}\")\n",
    "\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        enumerate(train_dataloader),\n",
    "        total=batches,\n",
    "        desc=f\"Epoch {epoch}\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for idx, batch in progress_bar:\n",
    "        batch_token_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss, logits = model(\n",
    "            batch_token_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=batch_attention_mask,\n",
    "            labels=batch_labels,\n",
    "            return_dict=False\n",
    "        )\n",
    "\n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update tqdm bar text with current loss\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    average_train_loss = training_loss / batches\n",
    "    print(f\"Average training loss: {average_train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e54e434",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "model.save_pretrained(\"movie_sentiment_model\")\n",
    "tokenizer.save_pretrained(\"movie_sentiment_model\")\n",
    "\n",
    "# Create a ZIP file for easier sharing\n",
    "shutil.make_archive(\"movie_sentiment_model\", 'zip', \"movie_sentiment_model\")\n",
    "print(\"Saved model and tokenizer to 'movie_sentiment_model/' and zipped as 'movie_sentiment_model.zip'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8655f67d",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# print(\"Loading model from drive...\");\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/movie_sentiment_model\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"/content/drive/MyDrive/movie_sentiment_model\")\n",
    "# model.to(device)\n",
    "\n",
    "print(\"Model ready\")\n",
    "\n",
    "def calculate_accuracy(preds, labels):\n",
    "    \"\"\" Calculate the accuracy of model predictions against true labels.\n",
    "\n",
    "    Parameters:\n",
    "        preds (np.array): The predicted label from the model\n",
    "        labels (np.array): The true label\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): The accuracy as a percentage of the correct\n",
    "            predictions.\n",
    "    \"\"\"\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "val_accuracy = 0\n",
    "\n",
    "total_batches = len(val_dataloader)\n",
    "print(f\"\\nValidation — total batches: {total_batches}\")\n",
    "\n",
    "progress_bar = tqdm(\n",
    "    enumerate(val_dataloader),\n",
    "    total=total_batches,\n",
    "    desc=\"Validating\",\n",
    "    position=0,\n",
    "    leave=True\n",
    ")\n",
    "\n",
    "for idx, batch in progress_bar:\n",
    "\n",
    "    batch_token_ids = batch[0].to(device)\n",
    "    batch_attention_mask = batch[1].to(device)\n",
    "    batch_labels = batch[2].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss, logits = model(\n",
    "            batch_token_ids,\n",
    "            attention_mask=batch_attention_mask,\n",
    "            labels=batch_labels,\n",
    "            token_type_ids=None,\n",
    "            return_dict=False\n",
    "        )\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = batch_labels.cpu().numpy()\n",
    "\n",
    "    val_loss += loss.item()\n",
    "    val_accuracy += calculate_accuracy(logits, label_ids)\n",
    "\n",
    "    # Update tqdm bar\n",
    "    progress_bar.set_postfix({\n",
    "        \"loss\": f\"{loss.item():.4f}\",\n",
    "        \"acc\": f\"{calculate_accuracy(logits, label_ids):.4f}\"\n",
    "    })\n",
    "\n",
    "average_val_loss = val_loss / total_batches\n",
    "average_val_accuracy = val_accuracy / total_batches\n",
    "\n",
    "print(f\"\\nAverage validation loss: {average_val_loss:.4f}\")\n",
    "print(f\"Average validation accuracy: {average_val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced2f50",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d384d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "review = \"Movie review goes here\"\n",
    "\n",
    "encoded = tokenizer.encode_plus(\n",
    "    review,\n",
    "    add_special_tokens=True,\n",
    "    max_length=128,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids = encoded['input_ids'].to(device)\n",
    "attention_mask = encoded['attention_mask'].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=None,\n",
    "        return_dict=False\n",
    "    )\n",
    "\n",
    "logits = outputs[0]              # shape: (1, num_labels)\n",
    "probs = F.softmax(logits, dim=1) # convert logits → probabilities\n",
    "\n",
    "pred_label = torch.argmax(probs, dim=1).item()\n",
    "confidence = probs[0][pred_label].item()\n",
    "\n",
    "label_names = [\"Negative\", \"Positive\"]\n",
    "\n",
    "print(f\"Prediction: {label_names[pred_label]}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
